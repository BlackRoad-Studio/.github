# HuggingFace Model Sync to Pi Fleet â€” Cost: $0
name: ï¿½ï¿½ HuggingFace Model Sync

on:
  workflow_dispatch:
    inputs:
      model:
        description: 'Model ID to pull (e.g., meta-llama/Llama-3.2-3B)'
        required: true
      target:
        description: 'Target Pi (octavia/alice/all)'
        required: false
        default: 'octavia'

jobs:
  sync-model:
    name: ðŸš€ Pull Model to ${{ github.event.inputs.target }}
    runs-on: [self-hosted, blackroad-fleet, octavia]
    steps:
      - name: ðŸ¤— Pull from HuggingFace
        env:
          HF_TOKEN: ${{ secrets.HUGGINGFACE_TOKEN }}
        run: |
          MODEL="${{ github.event.inputs.model }}"
          echo "Pulling $MODEL from HuggingFace..."
          
          # Use huggingface-cli if available
          if which huggingface-cli 2>/dev/null; then
            huggingface-cli download "$MODEL" --local-dir ~/models/$(basename $MODEL)
          else
            MODEL=$MODEL HF_TOKEN="${HF_TOKEN:-}" python3 -c \
              "import os,sys; from huggingface_hub import snapshot_download; m=os.environ['MODEL']; t=os.environ.get('HF_TOKEN'); p=snapshot_download(m,token=t,local_dir=os.path.expanduser(f'~/models/{os.path.basename(m)}')); print(f'Downloaded to: {p}')"
          fi
          
      - name: ðŸ¤– Convert to Ollama (if GGUF)
        run: |
          MODEL_DIR=~/models/$(basename ${{ github.event.inputs.model }})
          # Check if there's a GGUF file
          GGUF=$(find $MODEL_DIR -name "*.gguf" 2>/dev/null | head -1)
          if [ -n "$GGUF" ]; then
            MODEL_NAME=$(basename ${{ github.event.inputs.model }} | tr '[:upper:]' '[:lower:]')
            ollama create "hf-$MODEL_NAME" -f <(echo "FROM $GGUF")
            echo "âœ… Model registered in ollama as hf-$MODEL_NAME"
          fi
